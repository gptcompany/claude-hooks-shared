# Reusable Test Workflow
# Call from any repo: uses: gptprojectmanager/claude-hooks-shared/.github/workflows/_reusable-tests.yml@main

name: Tests

on:
  workflow_call:
    inputs:
      python-version:
        description: 'Python version to use'
        required: false
        type: string
        default: '3.12'
      test-paths:
        description: 'Paths to test'
        required: false
        type: string
        default: 'tests/'
      coverage-paths:
        description: 'Paths to measure coverage for'
        required: false
        type: string
        default: 'src/'
      coverage-threshold:
        description: 'Minimum coverage percentage'
        required: false
        type: number
        default: 80
      pytest-args:
        description: 'Additional pytest arguments'
        required: false
        type: string
        default: ''
    outputs:
      coverage:
        description: 'Coverage percentage'
        value: ${{ jobs.test.outputs.coverage }}
      tests_passed:
        description: 'Number of tests passed'
        value: ${{ jobs.test.outputs.tests_passed }}
      tests_failed:
        description: 'Number of tests failed'
        value: ${{ jobs.test.outputs.tests_failed }}

jobs:
  test:
    name: "Unit Tests"
    runs-on: [self-hosted, shared]
    timeout-minutes: 15

    outputs:
      coverage: ${{ steps.coverage.outputs.percent }}
      tests_passed: ${{ steps.tests.outputs.passed }}
      tests_failed: ${{ steps.tests.outputs.failed }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ inputs.python-version }}

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          enable-cache: true
          cache-dependency-glob: "**/pyproject.toml"

      - name: Install dependencies
        run: uv sync --all-extras

      - name: Run tests with coverage
        id: tests
        run: |
          set +e
          uv run pytest ${{ inputs.test-paths }} \
            -v --tb=short \
            --cov=${{ inputs.coverage-paths }} \
            --cov-report=json:coverage.json \
            --cov-report=term-missing \
            --cov-fail-under=${{ inputs.coverage-threshold }} \
            ${{ inputs.pytest-args }} \
            2>&1 | tee test_output.txt
          EXIT_CODE=${PIPESTATUS[0]}
          set -e

          PASSED=$(grep -oP '\d+(?= passed)' test_output.txt | tail -1 || echo 0)
          FAILED=$(grep -oP '\d+(?= failed)' test_output.txt | tail -1 || echo 0)

          echo "passed=$PASSED" >> $GITHUB_OUTPUT
          echo "failed=$FAILED" >> $GITHUB_OUTPUT

          if [ "$EXIT_CODE" -ne 0 ]; then
            echo "::error::Tests failed ($FAILED failures)"
            exit 1
          fi

          echo "Tests passed ($PASSED tests)"

      - name: Check coverage threshold
        id: coverage
        run: |
          if [ -f coverage.json ]; then
            COVERAGE=$(jq '.totals.percent_covered // 0' coverage.json)
            echo "percent=$COVERAGE" >> $GITHUB_OUTPUT

            if (( $(echo "$COVERAGE < ${{ inputs.coverage-threshold }}" | bc -l) )); then
              echo "::error::Coverage $COVERAGE% is below threshold ${{ inputs.coverage-threshold }}%"
              exit 1
            fi

            echo "Coverage: $COVERAGE% (threshold: ${{ inputs.coverage-threshold }}%)"
          else
            echo "::warning::No coverage report generated"
            echo "percent=0" >> $GITHUB_OUTPUT
          fi

      - name: Upload test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results
          path: |
            coverage.json
            test_output.txt
          retention-days: 7
