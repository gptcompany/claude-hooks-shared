---
phase: 03-lesson-learning
plan: 03
type: execute
wave: 2
depends_on: ["03-01", "03-02"]
files_modified: [tests/test_lesson_learning_integration.py]
autonomous: false
---

<objective>
Integration test and UAT verification for lesson learning system.

Purpose: Verify end-to-end flow from session → pattern extraction → pattern storage → lesson injection, and register hooks in settings.json.
Output: Working integration, hooks registered, verified lessons appear in additionalContext.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
@~/.claude/get-shit-done/references/checkpoints.md
</execution_context>

<context>
@/media/sam/1TB/claude-hooks-shared/.planning/PROJECT.md
@/media/sam/1TB/claude-hooks-shared/.planning/ROADMAP.md
@/media/sam/1TB/claude-hooks-shared/.planning/phases/03-lesson-learning/03-01-SUMMARY.md
@/media/sam/1TB/claude-hooks-shared/.planning/phases/03-lesson-learning/03-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create integration test</name>
  <files>tests/test_lesson_learning_integration.py</files>
  <action>
Create integration test that verifies full flow:

1. **Setup**: Seed trajectory data with high rework pattern
```python
# Seed trajectory index with problematic session
memory_store("trajectory:test-project:index", [
    {"id": "traj-001", "task": "edit config.py", "success": False, "steps": 5},
    {"id": "traj-002", "task": "edit config.py", "success": False, "steps": 3},
    {"id": "traj-003", "task": "edit config.py", "success": True, "steps": 4},
])
```

2. **Run meta_learning**: Execute Stop hook
```python
result = subprocess.run(
    ["python", "hooks/intelligence/meta_learning.py"],
    input=json.dumps({}),
    capture_output=True, text=True
)
```

3. **Verify patterns stored**: Check pattern exists
```python
patterns = pattern_search("config.py rework", top_k=3)
assert len(patterns) > 0
assert patterns[0]["type"] == "high_rework"
```

4. **Run lesson_injector**: Execute UserPromptSubmit hook
```python
result = subprocess.run(
    ["python", "hooks/intelligence/lesson_injector.py"],
    input=json.dumps({"prompt": "edit config.py", "cwd": "/test"}),
    capture_output=True, text=True
)
output = json.loads(result.stdout)
```

5. **Verify injection**: Check additionalContext contains lesson
```python
assert "additionalContext" in output
assert "config.py" in output["additionalContext"]
assert "Lessons" in output["additionalContext"]
```

Mark test with `@pytest.mark.integration` for selective running.
  </action>
  <verify>pytest tests/test_lesson_learning_integration.py -v -m integration passes</verify>
  <done>Integration test verifies full extraction → storage → injection flow</done>
</task>

<task type="auto">
  <name>Task 2: Register hooks in settings.json</name>
  <files>~/.claude/settings.json</files>
  <action>
Add hooks to settings.json using jq:

1. Add meta_learning.py to Stop hooks:
```bash
jq '.hooks.Stop[0].hooks += [{
  "type": "command",
  "command": "/media/sam/1TB/claude-hooks-shared/hooks/intelligence/meta_learning.py",
  "timeout": 10
}]' ~/.claude/settings.json > /tmp/settings.json && mv /tmp/settings.json ~/.claude/settings.json
```

2. Add lesson_injector.py to UserPromptSubmit hooks (with run_safe.py wrapper):
```bash
jq '.hooks.UserPromptSubmit[0].hooks += [{
  "type": "command",
  "command": "/media/sam/1TB/claude-hooks-shared/hooks/core/run_safe.py /media/sam/1TB/claude-hooks-shared/hooks/intelligence/lesson_injector.py",
  "timeout": 12
}]' ~/.claude/settings.json > /tmp/settings.json && mv /tmp/settings.json ~/.claude/settings.json
```

Verify registration:
```bash
jq '.hooks.Stop[0].hooks[] | select(.command | contains("meta_learning"))' ~/.claude/settings.json
jq '.hooks.UserPromptSubmit[0].hooks[] | select(.command | contains("lesson_injector"))' ~/.claude/settings.json
```
  </action>
  <verify>Both hooks appear in settings.json with correct paths and timeouts</verify>
  <done>Hooks registered in settings.json, will execute on next session</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Complete lesson learning system with hooks registered</what-built>
  <how-to-verify>
    1. Start a NEW Claude Code session (close current, open new terminal)
    2. Work on any task that involves editing files multiple times
    3. End the session (type "exit" or close terminal)
    4. Start another NEW session
    5. Check for "[Lessons]" in the hook output at session start

    Alternative quick verification:
    1. Manually seed a pattern:
       ```bash
       python -c "
       from hooks.core.mcp_client import pattern_store
       pattern_store(
         'File test.py was edited 5 times - consider smaller commits',
         'high_rework',
         0.9,
         {'file': 'test.py', 'edits': 5}
       )
       "
       ```
    2. Start new Claude session
    3. Type a prompt mentioning "test.py"
    4. Check if lesson appears in additionalContext
  </how-to-verify>
  <resume-signal>Type "verified" if lessons appear, or describe what's missing</resume-signal>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Integration test passes
- [ ] meta_learning.py registered in Stop hooks
- [ ] lesson_injector.py registered in UserPromptSubmit hooks
- [ ] Human verified lessons appear in new session
</verification>

<success_criteria>
- All verification criteria from ROADMAP Phase 3 met:
  - [ ] meta_learning.py extracts patterns (high rework, errors)
  - [ ] Patterns stored in claude-flow (pattern-store)
  - [ ] lesson_injector.py searches and injects patterns
  - [ ] Patterns appear in additionalContext
</success_criteria>

<output>
After completion, create `.planning/phases/03-lesson-learning/03-03-SUMMARY.md`

Update ROADMAP.md:
- Phase 3 status: **VERIFIED**
- Add verification results
</output>
