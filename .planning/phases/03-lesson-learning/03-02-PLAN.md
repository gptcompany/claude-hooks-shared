---
phase: 03-lesson-learning
plan: 02
type: execute
wave: 1
depends_on: []
files_modified: [hooks/intelligence/lesson_injector.py, tests/test_lesson_injector.py]
autonomous: true
---

<objective>
Implement lesson_injector.py UserPromptSubmit hook that injects relevant lessons.

Purpose: Search stored patterns and inject relevant lessons into additionalContext, enabling Claude to learn from past session mistakes without explicit user invocation.
Output: Working lesson_injector.py hook with tests, injecting lessons based on confidence thresholds.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@/media/sam/1TB/claude-hooks-shared/.planning/PROJECT.md
@/media/sam/1TB/claude-hooks-shared/.planning/ROADMAP.md
@/media/sam/1TB/claude-hooks-shared/.planning/phases/03-lesson-learning/03-CONTEXT.md

# Existing UserPromptSubmit hooks for patterns:
@/media/sam/1TB/claude-hooks-shared/hooks/intelligence/ci_status_injector.py
@/media/sam/1TB/claude-hooks-shared/hooks/session/session_restore_check.py
@/media/sam/1TB/claude-hooks-shared/hooks/core/mcp_client.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Write failing tests for lesson_injector.py</name>
  <files>tests/test_lesson_injector.py</files>
  <action>
Create test file with pytest tests covering:
1. `test_injects_high_confidence_lesson` - confidence >0.8 auto-injects
2. `test_suggests_medium_confidence_lesson` - confidence 0.5-0.8 suggests (different format)
3. `test_skips_low_confidence_lesson` - confidence <0.5 not injected
4. `test_searches_patterns_by_project` - uses project-specific search
5. `test_searches_patterns_by_context` - uses prompt context for relevance
6. `test_formats_additionalContext_correctly` - output format matches hook spec
7. `test_no_injection_when_no_patterns` - empty patterns = no additionalContext
8. `test_limits_injected_lessons` - max 3 lessons to avoid noise

Use pytest fixtures to mock:
- pattern_search (return sample patterns with various confidences)
- get_project_name (return test project)

Tests MUST fail initially (no implementation exists).

additionalContext format:
```
[Lessons from past sessions]
- (HIGH) File X was edited 5 times - consider smaller commits
- (MEDIUM) Consider: Error rate was high on similar task - use checkpoints
```
  </action>
  <verify>pytest tests/test_lesson_injector.py -v shows all tests FAIL (8 failures expected)</verify>
  <done>Test file exists with 8+ failing tests covering injection logic</done>
</task>

<task type="auto">
  <name>Task 2: Implement lesson_injector.py to pass tests</name>
  <files>hooks/intelligence/lesson_injector.py</files>
  <action>
Create UserPromptSubmit hook that:
1. Gets project name via `get_project_name()`
2. Extracts context from hook_input (prompt, cwd, etc.)
3. Searches patterns via `pattern_search(query, top_k=5, min_confidence=0.5)`
4. Filters and formats patterns by confidence:
   - HIGH (>0.8): Auto-inject with "[Lessons]" prefix
   - MEDIUM (0.5-0.8): Suggest with "Consider:" prefix
   - LOW (<0.5): Skip
5. Limits to max 3 lessons (avoid context pollution)
6. Returns additionalContext in hook output format:
```python
{
  "additionalContext": "[Lessons from past sessions]\n- (HIGH) ...\n- (MEDIUM) Consider: ..."
}
```
7. Logs to `/tmp/claude-metrics/lesson_injector.log`

Follow existing UserPromptSubmit hook patterns (see ci_status_injector.py):
- stdin JSON input with prompt, cwd
- stdout JSON output with additionalContext
- try/except with graceful failure (return empty dict)
- Use mcp_client imports with fallback

Query construction:
- Use project name + first 100 chars of prompt
- This enables semantic matching in pattern_search
  </action>
  <verify>pytest tests/test_lesson_injector.py -v shows all tests PASS</verify>
  <done>lesson_injector.py passes all tests, injects lessons based on confidence</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] pytest tests/test_lesson_injector.py passes all tests
- [ ] lesson_injector.py follows existing hook patterns (see ci_status_injector.py)
- [ ] Pattern search uses mcp_client.pattern_search()
- [ ] Confidence thresholds: HIGH >0.8, MEDIUM 0.5-0.8, LOW <0.5
- [ ] Manual test: `echo '{"prompt":"test","cwd":"/tmp"}' | python hooks/intelligence/lesson_injector.py` runs without error
</verification>

<success_criteria>
- All tests pass
- lesson_injector.py searches and injects relevant patterns
- Confidence-based filtering works correctly
- Hook output matches additionalContext format
</success_criteria>

<output>
After completion, create `.planning/phases/03-lesson-learning/03-02-SUMMARY.md`
</output>
