---
phase: 03-lesson-learning
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: [hooks/intelligence/meta_learning.py, tests/test_meta_learning.py]
autonomous: true
---

<objective>
Implement meta_learning.py Stop hook that extracts lessons from session data.

Purpose: Automatically capture learnable patterns (high rework, errors, quality drops) at session end and store them in claude-flow pattern-store for future injection.
Output: Working meta_learning.py hook with tests, integrated with existing session/trajectory data.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@/media/sam/1TB/claude-hooks-shared/.planning/PROJECT.md
@/media/sam/1TB/claude-hooks-shared/.planning/ROADMAP.md
@/media/sam/1TB/claude-hooks-shared/.planning/phases/03-lesson-learning/03-CONTEXT.md

# Existing hooks to understand patterns:
@/media/sam/1TB/claude-hooks-shared/hooks/intelligence/trajectory_tracker.py
@/media/sam/1TB/claude-hooks-shared/hooks/intelligence/session_analyzer.py
@/media/sam/1TB/claude-hooks-shared/hooks/session/session_checkpoint.py
@/media/sam/1TB/claude-hooks-shared/hooks/core/mcp_client.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Write failing tests for meta_learning.py</name>
  <files>tests/test_meta_learning.py</files>
  <action>
Create test file with pytest tests covering:
1. `test_extract_rework_pattern` - detects high rework (>3 edits on same file)
2. `test_extract_error_pattern` - detects high error rate (>25%)
3. `test_extract_quality_drop_pattern` - detects quality trend decline
4. `test_pattern_stored_with_confidence` - patterns stored via mcp_client.pattern_store()
5. `test_no_pattern_on_good_session` - no patterns extracted when session is healthy
6. `test_reads_trajectory_data` - reads from trajectory index
7. `test_reads_session_analyzer_data` - reads from session_analyzer output

Use pytest fixtures to mock:
- memory_retrieve (return sample trajectory data)
- pattern_store (verify called with correct args)

Tests MUST fail initially (no implementation exists).
  </action>
  <verify>pytest tests/test_meta_learning.py -v shows all tests FAIL (7 failures expected)</verify>
  <done>Test file exists with 7+ failing tests covering pattern extraction logic</done>
</task>

<task type="auto">
  <name>Task 2: Implement meta_learning.py to pass tests</name>
  <files>hooks/intelligence/meta_learning.py</files>
  <action>
Create Stop hook that:
1. Reads trajectory data: `memory_retrieve(f"trajectory:{project}:index")`
2. Reads session analyzer output: `/tmp/claude-metrics/session_analysis.json`
3. Extracts patterns based on thresholds:
   - Rework: >3 edits on same file → "high_rework" pattern
   - Errors: >25% error rate → "high_error" pattern
   - Quality: declining trend → "quality_drop" pattern
4. Calculates confidence score (0.0-1.0) based on:
   - Sample size (more data = higher confidence)
   - Pattern strength (worse metrics = higher confidence)
5. Stores patterns via `pattern_store(pattern, type, confidence, metadata)`
6. Logs to `/tmp/claude-metrics/meta_learning.log`

Follow existing hook patterns:
- argparse for --event argument (though Stop has single event)
- stdin JSON input, stdout JSON output
- try/except with graceful failure (return 0 always)
- Use mcp_client imports with fallback

Pattern format:
```python
{
  "pattern": "File X was edited 5 times - consider smaller commits",
  "type": "high_rework",
  "confidence": 0.85,
  "metadata": {"file": "X", "edits": 5, "project": "..."}
}
```
  </action>
  <verify>pytest tests/test_meta_learning.py -v shows all tests PASS</verify>
  <done>meta_learning.py passes all tests, extracts patterns from session data</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] pytest tests/test_meta_learning.py passes all tests
- [ ] meta_learning.py follows existing hook patterns (see trajectory_tracker.py)
- [ ] Pattern extraction uses mcp_client.pattern_store()
- [ ] Confidence scores calculated correctly
- [ ] Manual test: `echo '{}' | python hooks/intelligence/meta_learning.py` runs without error
</verification>

<success_criteria>
- All tests pass
- meta_learning.py extracts patterns from trajectory and session data
- Patterns stored with confidence scores
- Hook follows project conventions
</success_criteria>

<output>
After completion, create `.planning/phases/03-lesson-learning/03-01-SUMMARY.md`
</output>
